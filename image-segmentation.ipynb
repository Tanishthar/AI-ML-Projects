{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":1919543,"sourceType":"datasetVersion","datasetId":1144726}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Segmentation ( Make sure you are using the T4 GPU Runtime )","metadata":{}},{"cell_type":"markdown","source":"This task focuses on <a href=\"https://www.v7labs.com/blog/image-segmentation-guide\" class=\"external\">Image Segmentation</a> and Object Detection.\n\n## What is image segmentation?\n\nIn an image classification task, the network assigns a label (or class) to each input image. However, suppose you want to know the shape of that object, which pixel belongs to which object, etc. In this case, you need to assign a class to each pixel of the image—this task is known as segmentation. A segmentation model returns much more detailed information about the image. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging, just to name a few.\n\nThis task uses the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/ ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). The dataset consists of images of 37 pet breeds, with 200 images per breed (~100 each in the training and test splits). Each image includes the corresponding labels, and pixel-wise masks. The masks are class-labels for each pixel. Each pixel is given one of three categories:\n\n- Class 1: Pixel belonging to the pet.\n- Class 2: Pixel bordering the pet.\n- Class 3: None of the above/a surrounding pixel.","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{}},{"cell_type":"code","source":"!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n!tar -xf images.tar.gz\n!tar -xf annotations.tar.gz","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:21:54.825826Z","iopub.execute_input":"2024-10-19T09:21:54.826115Z","iopub.status.idle":"2024-10-19T09:23:04.054128Z","shell.execute_reply.started":"2024-10-19T09:21:54.826079Z","shell.execute_reply":"2024-10-19T09:23:04.052851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:23:04.05665Z","iopub.execute_input":"2024-10-19T09:23:04.057097Z","iopub.status.idle":"2024-10-19T09:23:26.310363Z","shell.execute_reply.started":"2024-10-19T09:23:04.057045Z","shell.execute_reply":"2024-10-19T09:23:26.309074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.utils import array_to_img\nfrom tensorflow.keras.utils import load_img, img_to_array\nfrom tensorflow_examples.models.pix2pix import pix2pix\nimport numpy as np\nimport random\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:23:26.31253Z","iopub.execute_input":"2024-10-19T09:23:26.313083Z","iopub.status.idle":"2024-10-19T09:23:39.03689Z","shell.execute_reply.started":"2024-10-19T09:23:26.312994Z","shell.execute_reply":"2024-10-19T09:23:39.036104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"input_dir = \"/kaggle/working/images\"\ntarget_dir = \"/kaggle/working/annotations/trimaps/\"\n\n# Loading Image Paths\ninput_img_paths = sorted(\n    [os.path.join(input_dir, fname)\n     for fname in os.listdir(input_dir)\n     if fname.endswith(\".jpg\")])\ntarget_paths = sorted(\n    [os.path.join(target_dir, fname)\n     for fname in os.listdir(target_dir)\n     if fname.endswith(\".png\") and not fname.startswith(\".\")])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-19T09:23:39.039114Z","iopub.execute_input":"2024-10-19T09:23:39.039645Z","iopub.status.idle":"2024-10-19T09:23:39.10421Z","shell.execute_reply.started":"2024-10-19T09:23:39.03961Z","shell.execute_reply":"2024-10-19T09:23:39.103498Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking if the Paths loaded correctly (if yes, you will see a brown cat)\nplt.axis(\"off\")\nplt.imshow(load_img(input_img_paths[9]))","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:23:39.105141Z","iopub.execute_input":"2024-10-19T09:23:39.105415Z","iopub.status.idle":"2024-10-19T09:23:39.553241Z","shell.execute_reply.started":"2024-10-19T09:23:39.105383Z","shell.execute_reply":"2024-10-19T09:23:39.552312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for Displaying Masked Image (Segmented Image)\ndef display_target(target_array):\n    normalized_array = (target_array.astype(\"uint8\") - 1) * 127 # Normalizing the Image\n    plt.axis(\"off\")\n    plt.imshow(normalized_array[:, :, 0])\n\nimg = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\")) # Loading a single image\ndisplay_target(img)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:23:39.554327Z","iopub.execute_input":"2024-10-19T09:23:39.554628Z","iopub.status.idle":"2024-10-19T09:23:39.68962Z","shell.execute_reply.started":"2024-10-19T09:23:39.554595Z","shell.execute_reply":"2024-10-19T09:23:39.68816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading entire Dataset\nimg_size = (200, 200) # set it as 200x200 ka tuple\nnum_imgs = len(input_img_paths)\n\nrandom.Random(1337).shuffle(input_img_paths) # Shuffle the Paths (input_img_paths and target_paths)\nrandom.Random(1337).shuffle(target_paths)\n\n# Create function to load input image from path\ndef path_to_input_image(path):\n    return img_to_array(load_img(path, target_size=img_size))\n\n# Create function to load target image from path\ndef path_to_target(path):\n    img = img_to_array(\n        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n    img = img.astype(\"uint8\") - 1\n    return img\n\ninput_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\ntargets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\nfor i in range(num_imgs):\n    input_imgs[i] = path_to_input_image(input_img_paths[i])\n    targets[i] = path_to_target(target_paths[i])\n\nnum_val_samples = 1000 # Play around with it if want to\ntrain_input_imgs = input_imgs[:-num_val_samples]\ntrain_targets = targets[:-num_val_samples]\nval_input_imgs = input_imgs[-num_val_samples:]\nval_targets = targets[-num_val_samples:]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:23:39.691619Z","iopub.execute_input":"2024-10-19T09:23:39.692224Z","iopub.status.idle":"2024-10-19T09:24:07.063606Z","shell.execute_reply.started":"2024-10-19T09:23:39.692158Z","shell.execute_reply":"2024-10-19T09:24:07.062793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_targets[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:24:07.064689Z","iopub.execute_input":"2024-10-19T09:24:07.064978Z","iopub.status.idle":"2024-10-19T09:24:07.071327Z","shell.execute_reply.started":"2024-10-19T09:24:07.064947Z","shell.execute_reply":"2024-10-19T09:24:07.07049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model\n\nA Image Segmentation Model, at its core, typically comprises a convolutional neural network (CNN) architecture, leveraging its ability to capture hierarchical features in an image. The model consists of encoder or Downsampler and decoder or Upsampler components, where the encoder extracts relevant features from the input image, and the decoder reconstructs the segmented output.\n\nEach Conv2D layer in the encoder applies filters to the input image, extracting low-level to high-level features. As you move deeper into the encoder, the spatial dimensions of the feature maps typically decrease while the depth (number of channels) increases.\n\nConv2DTranspose layers are employed in the decoder to upsample the feature maps, gradually increasing the spatial dimensions while reducing the number of channels. Skip connections are often added between corresponding encoder and decoder layers to preserve fine-grained details.","metadata":{}},{"cell_type":"code","source":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n    x = layers.Rescaling(1./255)(inputs) # Create a Rescaling layer to convert the range [0,255] to [0,1]\n\n    # Similarly add 6 Conv2D Layers with Relu Activation, Strides as required in alternate layers (for layers with even index) (Try khudse) and padding so that output size is equal to input size.\n    # This Block acts as our Encoder Block to understand features of the Input Image\n    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n\n    # Now add 6 Conv2DTranspose Layers with Relu, strides in alternate layers (for layers with odd indexes) and same padding.\n    '''This Block acts as our Decoder Block to regenerate the Image from the Convoluted Feature Map. This allows for linking and expanding\n    the Segmented Features to the original Image.'''\n    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n\n    # Finally, add a softmax with same padding to get the final Segmented Image\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Create the Model using the Model API\n    model = keras.Model(inputs, outputs)\n    return model\n\nmodel = get_model(img_size=img_size, num_classes=3) # Why num_classes = 3 here? Batao Sochke\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:24:07.072881Z","iopub.execute_input":"2024-10-19T09:24:07.073639Z","iopub.status.idle":"2024-10-19T09:24:08.008055Z","shell.execute_reply.started":"2024-10-19T09:24:07.073596Z","shell.execute_reply":"2024-10-19T09:24:08.007182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # Use appropriate Loss function (Hint: Images use kar rhe hai toh konsa use hoga?)\n# Callbacks: https://keras.io/api/callbacks/\n# We are saving this model after each epoch so that bichme agar band hogaya toh bhi we have somewhere to start with\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n                                    save_best_only=True, verbose=2)\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:24:08.012268Z","iopub.execute_input":"2024-10-19T09:24:08.01264Z","iopub.status.idle":"2024-10-19T09:24:08.025784Z","shell.execute_reply.started":"2024-10-19T09:24:08.0126Z","shell.execute_reply":"2024-10-19T09:24:08.024756Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and Testing","metadata":{}},{"cell_type":"code","source":"# Train the Model for 3 epochs with the batch size of 64, make sure to set the callbacks parameter\nhistory = model.fit(train_input_imgs, train_targets,\n                    epochs=3,\n                    callbacks=callbacks,\n                    batch_size=64,\n                    validation_data=(val_input_imgs, val_targets))","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:24:08.026899Z","iopub.execute_input":"2024-10-19T09:24:08.027182Z","iopub.status.idle":"2024-10-19T09:28:46.591023Z","shell.execute_reply.started":"2024-10-19T09:24:08.027151Z","shell.execute_reply":"2024-10-19T09:28:46.59012Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting Loss\nepochs = range(1, len(history.history[\"loss\"]) + 1)\nloss = history.history[\"loss\"] # history me se Loss nikalo\nval_loss = history.history[\"val_loss\"] # history me se validation loss nikalo\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:46.592475Z","iopub.execute_input":"2024-10-19T09:28:46.592832Z","iopub.status.idle":"2024-10-19T09:28:46.93089Z","shell.execute_reply.started":"2024-10-19T09:28:46.592795Z","shell.execute_reply":"2024-10-19T09:28:46.930002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:46.932157Z","iopub.execute_input":"2024-10-19T09:28:46.932476Z","iopub.status.idle":"2024-10-19T09:28:47.154792Z","shell.execute_reply.started":"2024-10-19T09:28:46.932444Z","shell.execute_reply":"2024-10-19T09:28:47.153792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Saved Model using load_model\nmodel = keras.models.load_model(\"oxford_segmentation.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:47.157976Z","iopub.execute_input":"2024-10-19T09:28:47.15849Z","iopub.status.idle":"2024-10-19T09:28:47.806709Z","shell.execute_reply.started":"2024-10-19T09:28:47.158455Z","shell.execute_reply":"2024-10-19T09:28:47.80589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test Model\ndef predict(i):\n  test_image = val_input_imgs[i]\n  plt.axis(\"off\")\n  plt.imshow(array_to_img(test_image))\n  mask = model.predict(np.expand_dims(test_image, 0)) # Predict karo. PS: Look carefully at the dimensions.\n  display_mask(mask[0])\n\ndef display_mask(pred):\n    mask = np.argmax(pred, axis=-1)\n    mask *= 127\n    plt.axis(\"off\")\n    plt.imshow(mask)\n\n# Use or Modify the above Functions to display 20 test images in the SAME PLOT\npredict(100)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:47.815183Z","iopub.execute_input":"2024-10-19T09:28:47.815503Z","iopub.status.idle":"2024-10-19T09:28:50.012514Z","shell.execute_reply.started":"2024-10-19T09:28:47.815468Z","shell.execute_reply":"2024-10-19T09:28:50.011075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Garbage Collection so that aage wale parts me issue nhi aaye\n\ndel(model,train_input_imgs, train_targets, val_input_imgs, val_targets, history, input_imgs, targets)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:50.014443Z","iopub.execute_input":"2024-10-19T09:28:50.015029Z","iopub.status.idle":"2024-10-19T09:28:50.466049Z","shell.execute_reply.started":"2024-10-19T09:28:50.014967Z","shell.execute_reply":"2024-10-19T09:28:50.46497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So now, we implemented a Custom Model for Image Segmentation. Abhi there are a few existing architectures which efficiently do this for us. For example: U-Net, Mask R-CNN, Fast FCN etc.\n\nYou can read more about them here:\nhttps://neptune.ai/blog/image-segmentation#:~:text=The%20basic%20architecture%20in%20image,an%20encoder%20and%20a%20decoder.&text=The%20encoder%20extracts%20features%20from,the%20outline%20of%20the%20object.\n\n\n\nWe will now implement U-Net Architecture. It is similar to the one we created above.\nhttps://www.geeksforgeeks.org/u-net-architecture-explained/","metadata":{}},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:28:50.467113Z","iopub.execute_input":"2024-10-19T09:28:50.467387Z","iopub.status.idle":"2024-10-19T09:30:34.566654Z","shell.execute_reply.started":"2024-10-19T09:28:50.467356Z","shell.execute_reply":"2024-10-19T09:30:34.565606Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) / 255.0\n  input_mask -= 1\n  return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.56791Z","iopub.execute_input":"2024-10-19T09:30:34.568224Z","iopub.status.idle":"2024-10-19T09:30:34.573078Z","shell.execute_reply.started":"2024-10-19T09:30:34.56819Z","shell.execute_reply":"2024-10-19T09:30:34.572147Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128)) # Use tf.image to resize to (128,128)\n  input_mask = tf.image.resize( # Use Nearest Neighbour Method to resize to (128,128) for the image mask\n    datapoint['segmentation_mask'],\n    (128, 128),\n    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n  )\n\n  input_image, input_mask = normalize(input_image, input_mask) # Normalize the Images\n\n  return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.574459Z","iopub.execute_input":"2024-10-19T09:30:34.575037Z","iopub.status.idle":"2024-10-19T09:30:34.58415Z","shell.execute_reply.started":"2024-10-19T09:30:34.574991Z","shell.execute_reply":"2024-10-19T09:30:34.583062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset already contains the required training and test splits, so continue to use the same splits:","metadata":{}},{"cell_type":"code","source":"TRAIN_LENGTH = info.splits['train'].num_examples\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.585346Z","iopub.execute_input":"2024-10-19T09:30:34.58566Z","iopub.status.idle":"2024-10-19T09:30:34.599473Z","shell.execute_reply.started":"2024-10-19T09:30:34.585628Z","shell.execute_reply":"2024-10-19T09:30:34.598592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\ntest_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.600554Z","iopub.execute_input":"2024-10-19T09:30:34.600862Z","iopub.status.idle":"2024-10-19T09:30:34.73969Z","shell.execute_reply.started":"2024-10-19T09:30:34.600831Z","shell.execute_reply":"2024-10-19T09:30:34.738934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following class performs a simple augmentation by randomly-flipping an image.\n\n[Image augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)\n","metadata":{}},{"cell_type":"code","source":"class Augment(tf.keras.layers.Layer):\n  def __init__(self, seed=42):\n    super().__init__()\n    # both use the same seed, so they'll make the same random changes.\n    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed) # Do the same for this\n\n  def call(self, inputs, labels):\n    inputs = self.augment_inputs(inputs)\n    labels = self.augment_labels(labels)\n    return inputs, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.740809Z","iopub.execute_input":"2024-10-19T09:30:34.741101Z","iopub.status.idle":"2024-10-19T09:30:34.747177Z","shell.execute_reply.started":"2024-10-19T09:30:34.741069Z","shell.execute_reply":"2024-10-19T09:30:34.746252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build the input pipeline, applying the augmentation after batching the inputs:","metadata":{}},{"cell_type":"code","source":"train_batches = (\n    train_images\n    .cache()\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .map(Augment())\n    .prefetch(buffer_size=tf.data.AUTOTUNE)) # Prefetch is an important step towards training efficiency as it allows the Model to Fetch the Next Batch of Data while Training on a Different Batch\n    # AUTOTUNE allows Tensorflow to automatically adjust the prefetch buffer. Use that.\n\ntest_batches = test_images.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.748354Z","iopub.execute_input":"2024-10-19T09:30:34.748648Z","iopub.status.idle":"2024-10-19T09:30:34.854447Z","shell.execute_reply.started":"2024-10-19T09:30:34.748617Z","shell.execute_reply":"2024-10-19T09:30:34.853772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize an image example and its corresponding mask from the dataset:","metadata":{}},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.855547Z","iopub.execute_input":"2024-10-19T09:30:34.855915Z","iopub.status.idle":"2024-10-19T09:30:34.862048Z","shell.execute_reply.started":"2024-10-19T09:30:34.855872Z","shell.execute_reply":"2024-10-19T09:30:34.861128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the train_batches to obtain 2 images and display them using the function above\nfor images, masks in train_batches.take(2):\n  sample_image, sample_mask = images[0], masks[0]\n  display([sample_image, sample_mask])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:34.863098Z","iopub.execute_input":"2024-10-19T09:30:34.863639Z","iopub.status.idle":"2024-10-19T09:30:37.420849Z","shell.execute_reply.started":"2024-10-19T09:30:34.863587Z","shell.execute_reply":"2024-10-19T09:30:37.419865Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define the model\n\nThe model being used here is a modified [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). To learn robust features and reduce the number of trainable parameters, use a pretrained model—[MobileNetV2](https://arxiv.org/abs/1801.04381)—as the encoder. For the decoder, you will use the upsample block, which is already implemented in the [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) example in the TensorFlow Examples repo.","metadata":{}},{"cell_type":"markdown","source":"As mentioned, the encoder is a pretrained MobileNetV2 model. You will use the model from `tf.keras.applications`. The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process.","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False) # Load the MobileNetV2 Pre-trained model without its final Dense Layers\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model using the tf.keras.Model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\n# Since this acts as our pre-trained encoder block, make sure to set the layers to be non-trainable\ndown_stack.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:37.422454Z","iopub.execute_input":"2024-10-19T09:30:37.422822Z","iopub.status.idle":"2024-10-19T09:30:39.995262Z","shell.execute_reply.started":"2024-10-19T09:30:37.422785Z","shell.execute_reply":"2024-10-19T09:30:39.994226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples:","metadata":{}},{"cell_type":"code","source":"# Use pix2pix upsample layers to get our unsampling block\nup_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:39.9965Z","iopub.execute_input":"2024-10-19T09:30:39.996906Z","iopub.status.idle":"2024-10-19T09:30:40.015655Z","shell.execute_reply.started":"2024-10-19T09:30:39.996861Z","shell.execute_reply":"2024-10-19T09:30:40.014633Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unet_model(output_channels:int):\n  inputs = tf.keras.layers.Input(shape=[128, 128, 3]) # Create an Input Layer of shape (128,128,3)\n\n  # Downsampling through the model\n  skips = down_stack(inputs)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    concat = tf.keras.layers.Concatenate() # Concatenate X and Skip Layers\n    x = concat([x, skip])\n\n  # This is the last layer of the model\n  last = tf.keras.layers.Conv2DTranspose(\n      filters=output_channels, kernel_size=3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:40.024769Z","iopub.execute_input":"2024-10-19T09:30:40.025279Z","iopub.status.idle":"2024-10-19T09:30:40.032306Z","shell.execute_reply.started":"2024-10-19T09:30:40.025246Z","shell.execute_reply":"2024-10-19T09:30:40.031386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note that the number of filters on the last layer is set to the number of `output_channels`. This will be one output channel per class.","metadata":{}},{"cell_type":"markdown","source":"## Train the model\n\nNow, all that is left to do is to compile and train the model.\n\nSince this is a multiclass classification problem, use the `tf.keras.losses.SparseCategoricalCrossentropy` loss function with the `from_logits` argument set to `True`, since the labels are scalar integers instead of vectors of scores for each pixel of every class.\n\nWhen running inference, the label assigned to the pixel is the channel with the highest value. This is what the `create_mask` function is doing.","metadata":{}},{"cell_type":"code","source":"OUTPUT_CLASSES = 3\n\nmodel = unet_model(output_channels=OUTPUT_CLASSES)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:40.033436Z","iopub.execute_input":"2024-10-19T09:30:40.033851Z","iopub.status.idle":"2024-10-19T09:30:40.123825Z","shell.execute_reply.started":"2024-10-19T09:30:40.033777Z","shell.execute_reply":"2024-10-19T09:30:40.123088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot the resulting model architecture:","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:40.124767Z","iopub.execute_input":"2024-10-19T09:30:40.125069Z","iopub.status.idle":"2024-10-19T09:30:41.272609Z","shell.execute_reply.started":"2024-10-19T09:30:40.125038Z","shell.execute_reply":"2024-10-19T09:30:41.271423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Try out the model to check what it predicts before training:","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask):\n  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:41.274106Z","iopub.execute_input":"2024-10-19T09:30:41.274965Z","iopub.status.idle":"2024-10-19T09:30:41.280717Z","shell.execute_reply.started":"2024-10-19T09:30:41.274905Z","shell.execute_reply":"2024-10-19T09:30:41.279691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:41.282116Z","iopub.execute_input":"2024-10-19T09:30:41.282749Z","iopub.status.idle":"2024-10-19T09:30:44.047296Z","shell.execute_reply.started":"2024-10-19T09:30:41.282691Z","shell.execute_reply":"2024-10-19T09:30:44.045992Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_predictions()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:44.048814Z","iopub.execute_input":"2024-10-19T09:30:44.049228Z","iopub.status.idle":"2024-10-19T09:30:48.879625Z","shell.execute_reply.started":"2024-10-19T09:30:44.049178Z","shell.execute_reply":"2024-10-19T09:30:48.878681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    show_predictions()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:48.88089Z","iopub.execute_input":"2024-10-19T09:30:48.881196Z","iopub.status.idle":"2024-10-19T09:30:48.889548Z","shell.execute_reply.started":"2024-10-19T09:30:48.881163Z","shell.execute_reply":"2024-10-19T09:30:48.885309Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 20\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n\nmodel_history = model.fit(train_batches, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_batches,\n                          callbacks=[DisplayCallback()])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:30:48.890869Z","iopub.execute_input":"2024-10-19T09:30:48.891421Z","iopub.status.idle":"2024-10-19T09:33:49.226361Z","shell.execute_reply.started":"2024-10-19T09:30:48.89138Z","shell.execute_reply":"2024-10-19T09:33:49.225506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Loss and Validation Loss as previously done\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nplt.figure()\nplt.plot(model_history.epoch, loss, 'r', label='Training loss')\nplt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:33:49.227728Z","iopub.execute_input":"2024-10-19T09:33:49.2281Z","iopub.status.idle":"2024-10-19T09:33:49.512515Z","shell.execute_reply.started":"2024-10-19T09:33:49.228064Z","shell.execute_reply":"2024-10-19T09:33:49.511681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make predictions\n\nNow, make some predictions.","metadata":{}},{"cell_type":"code","source":"show_predictions(test_batches, 3)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:33:49.513609Z","iopub.execute_input":"2024-10-19T09:33:49.513944Z","iopub.status.idle":"2024-10-19T09:33:53.856138Z","shell.execute_reply.started":"2024-10-19T09:33:49.513909Z","shell.execute_reply":"2024-10-19T09:33:53.855133Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Detection","metadata":{}},{"cell_type":"markdown","source":"Object detection is a computer vision technique for locating instances of objects in images or videos.\n\nRead more:\n\nhttps://www.mathworks.com/discovery/object-detection.html\n\nhttps://www.analyticsvidhya.com/blog/2022/03/a-basic-introduction-to-object-detection/","metadata":{}},{"cell_type":"markdown","source":"In this, we will not use one of those high performing off-the-shelf object detectors but develop a new one ourselves, from scratch, using plain python, OpenCV, and Tensorflow.\n\nAn Object Detection is a combination of two tasks:\n*   Regression of the bound-box coordinates\n*   Classification of the object label\n\nThis means that our model has two outputs: namely the object label and the object bound box. Therefore, the model must combine the tasks of classification and regression.","metadata":{}},{"cell_type":"markdown","source":"The Dataset we will be using is:\n\nhttps://www.kaggle.com/datasets/techzizou/labeled-mask-dataset-yolo-darknet","metadata":{}},{"cell_type":"code","source":"import os, random\nimport tensorflow as tf\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:33:53.857458Z","iopub.execute_input":"2024-10-19T09:33:53.85782Z","iopub.status.idle":"2024-10-19T09:33:53.862723Z","shell.execute_reply.started":"2024-10-19T09:33:53.857727Z","shell.execute_reply":"2024-10-19T09:33:53.861907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"markdown","source":"This code basically generates three lists: one for training (holding 70% of the data), one for validation (20% of the data) and one for test (the last 10%). The code also shuffle the data in order to avoid any natural bias.\n\nNote that we are removing the images with more than one masks/objects. This is because we are building a simple object detector able to detect only a single object in an image.","metadata":{}},{"cell_type":"code","source":"def list_files(full_data_path = \"/kaggle/input/labeled-mask-dataset-yolo-darknet/obj/\", image_ext = '.jpg', split_percentage = [70, 20]):\n\n    files = []\n\n    discarded = 0\n    masked_instance = 0\n\n    for r, d, f in os.walk(full_data_path):\n        for file in f:\n            if file.endswith(\".txt\"):\n\n                # first, let's check if there is only one object\n                with open(full_data_path + \"/\" + file, 'r') as fp:\n                    lines = fp.readlines()\n                    if len(lines) > 1:\n                        discarded += 1\n                        continue\n\n\n                strip = file[0:len(file) - len(\".txt\")]\n                # secondly, check if the paired image actually exist\n                image_path = full_data_path + \"/\" + strip + image_ext\n                if os.path.isfile(image_path):\n                    # checking the class. '0' means masked, '1' for unmasked\n                    if lines[0][0] == '0':\n                        masked_instance += 1\n                    files.append(strip)\n\n    size = len(files)\n    print(str(discarded) + \" file(s) discarded\")\n    print(str(size) + \" valid case(s)\")\n    print(str(masked_instance) + \" are masked cases\")\n\n    random.shuffle(files)\n\n    split_training = int(split_percentage[0] * size / 100)\n    split_validation = split_training + int(split_percentage[1] * size / 100)\n\n    return files[0:split_training], files[split_training:split_validation], files[split_validation:]\n\ntraining_files, validation_files, test_files = list_files()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:33:53.864096Z","iopub.execute_input":"2024-10-19T09:33:53.864379Z","iopub.status.idle":"2024-10-19T09:34:07.853402Z","shell.execute_reply.started":"2024-10-19T09:33:53.864348Z","shell.execute_reply":"2024-10-19T09:34:07.852437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(str(len(training_files)) + \" training files\")\nprint(str(len(validation_files)) + \" validation files\")\nprint(str(len(test_files)) + \" test files\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:07.855037Z","iopub.execute_input":"2024-10-19T09:34:07.855416Z","iopub.status.idle":"2024-10-19T09:34:07.861108Z","shell.execute_reply.started":"2024-10-19T09:34:07.855373Z","shell.execute_reply":"2024-10-19T09:34:07.860134Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since our model will be using a fixed 244 x 244 input layer, we need to format any input image before feed it to the model (to train or to predict).","metadata":{}},{"cell_type":"code","source":"input_size = 244\n\ndef format_image(img, box):\n  '''\n  1. Get the height and width of the input image.\n  2. Calculate the maximum dimension of the image (either height or width).\n  3. Calculate the scaling ratio needed to fit the image within the desired input_size.\n  4. Calculate the new width and height of the image based on the scaling ratio.\n  5. Create a tuple new_size containing the new width and height.\n  6. Resize the original image to the new dimensions using linear interpolation.\n  7. Create a new image (new_image) of size input_size x input_size initialized with zeros.\n  8. Place the resized image in the upper-left corner of new_image, filling the remaining space with zeros.\n  '''\n  height, width = img.shape\n  max_size = max(height, width)\n  r = max_size / input_size\n  new_width = int(width / r)\n  new_height = int(height / r)\n  new_size = (new_width, new_height)\n  resized = cv.resize(img, new_size, interpolation= cv.INTER_LINEAR)\n  new_image = np.zeros((input_size, input_size), dtype=np.uint8)\n  new_image[0:new_height, 0:new_width] = resized\n\n  x, y, w, h = box[0], box[1], box[2], box[3]\n\n  # Calculate the new coordinates and dimensions (new_box) based on the scaling ratio (r)\n  new_box = [int((x - 0.5*w)* width / r), int((y - 0.5*h) * height / r), int(w*width / r), int(h*height / r)]\n\n  return new_image, new_box","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:07.862258Z","iopub.execute_input":"2024-10-19T09:34:07.862572Z","iopub.status.idle":"2024-10-19T09:34:07.873749Z","shell.execute_reply.started":"2024-10-19T09:34:07.86253Z","shell.execute_reply":"2024-10-19T09:34:07.872853Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_load(files, full_data_path = \"/kaggle/input/labeled-mask-dataset-yolo-darknet/obj/\", image_ext = \".jpg\"):\n    X = []\n    Y = []\n\n    for file in files:\n        img = cv.imread(os.path.join(full_data_path, file + image_ext), cv.IMREAD_GRAYSCALE)\n\n        k = 1\n\n        with open(full_data_path + \"/\" + file + \".txt\", 'r') as fp:\n            line = fp.readlines()[0]\n            if line[0] == '0':\n                k = 0\n\n            box = np.array(line[1:].split(), dtype=float)\n\n        img, box = format_image(img, box)\n        img = img.astype(float) / 255.\n        box = np.asarray(box, dtype=float) / input_size\n        label = np.append(box, k)\n\n        X.append(img)\n        Y.append(label)\n\n    X = np.array(X)\n\n    X = np.expand_dims(X, axis=3)\n\n    X = tf.convert_to_tensor(X, dtype=tf.float32) # Convert X and Y to tensors with dtype as float32\n\n    Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n\n    result = tf.data.Dataset.from_tensor_slices((X, Y))\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:07.87483Z","iopub.execute_input":"2024-10-19T09:34:07.875128Z","iopub.status.idle":"2024-10-19T09:34:07.889464Z","shell.execute_reply.started":"2024-10-19T09:34:07.875097Z","shell.execute_reply":"2024-10-19T09:34:07.888616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_train_ds = data_load(training_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:07.890511Z","iopub.execute_input":"2024-10-19T09:34:07.890821Z","iopub.status.idle":"2024-10-19T09:34:19.606483Z","shell.execute_reply.started":"2024-10-19T09:34:07.890789Z","shell.execute_reply":"2024-10-19T09:34:19.605571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_validation_ds = data_load(validation_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:19.607818Z","iopub.execute_input":"2024-10-19T09:34:19.608191Z","iopub.status.idle":"2024-10-19T09:34:23.19829Z","shell.execute_reply.started":"2024-10-19T09:34:19.608145Z","shell.execute_reply":"2024-10-19T09:34:23.197243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_test_ds = data_load(test_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:23.199453Z","iopub.execute_input":"2024-10-19T09:34:23.199797Z","iopub.status.idle":"2024-10-19T09:34:24.792999Z","shell.execute_reply.started":"2024-10-19T09:34:23.199748Z","shell.execute_reply":"2024-10-19T09:34:24.792216Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since our model must implement two tasks — classification and regression — we need two different Loss Functions:\n\n* One for the classification task: we may use any Loss Function usually found in only-classification tasks like Categorical Crossentropy.\n* One for the bound box regression: we can use a regression Loss Function such as Mean Squared Error.\n\n","metadata":{}},{"cell_type":"code","source":"CLASSES = 2\n\ndef format_instance(image, label):\n    return image, (tf.one_hot(int(label[4]), CLASSES), [label[0], label[1], label[2], label[3]])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.79414Z","iopub.execute_input":"2024-10-19T09:34:24.794438Z","iopub.status.idle":"2024-10-19T09:34:24.799473Z","shell.execute_reply.started":"2024-10-19T09:34:24.794407Z","shell.execute_reply":"2024-10-19T09:34:24.798637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\n\n# see https://www.tensorflow.org/guide/data_performance\n\ndef tune_training_ds(dataset):\n    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n    dataset = dataset.repeat() # The dataset be repeated indefinitely.\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.800865Z","iopub.execute_input":"2024-10-19T09:34:24.801306Z","iopub.status.idle":"2024-10-19T09:34:24.812653Z","shell.execute_reply.started":"2024-10-19T09:34:24.801264Z","shell.execute_reply":"2024-10-19T09:34:24.8118Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = tune_training_ds(raw_train_ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.813784Z","iopub.execute_input":"2024-10-19T09:34:24.814457Z","iopub.status.idle":"2024-10-19T09:34:24.899256Z","shell.execute_reply.started":"2024-10-19T09:34:24.814414Z","shell.execute_reply":"2024-10-19T09:34:24.898572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tune_validation_ds(dataset):\n    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(len(validation_files) // 4)\n    dataset = dataset.repeat()\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.9003Z","iopub.execute_input":"2024-10-19T09:34:24.900651Z","iopub.status.idle":"2024-10-19T09:34:24.90568Z","shell.execute_reply.started":"2024-10-19T09:34:24.900608Z","shell.execute_reply":"2024-10-19T09:34:24.904812Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_ds = tune_validation_ds(raw_validation_ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.906852Z","iopub.execute_input":"2024-10-19T09:34:24.907227Z","iopub.status.idle":"2024-10-19T09:34:24.944596Z","shell.execute_reply.started":"2024-10-19T09:34:24.907177Z","shell.execute_reply":"2024-10-19T09:34:24.943765Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tune_test_ds(dataset):\n    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(1)\n    dataset = dataset.repeat()\n    return dataset\n\ntest_ds = tune_test_ds(raw_test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.945797Z","iopub.execute_input":"2024-10-19T09:34:24.946156Z","iopub.status.idle":"2024-10-19T09:34:24.977716Z","shell.execute_reply.started":"2024-10-19T09:34:24.946114Z","shell.execute_reply":"2024-10-19T09:34:24.977066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model ;)","metadata":{}},{"cell_type":"code","source":"def build_feature_extractor(inputs):\n    # use 3 pairs of Conv2D and AveragePooling2D with relu activation and kernel size = 3. Keep in mind we will be using\n    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n\n    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu')(x)\n    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n\n    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu')(x)\n    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n\n    return x\n\ndef build_model_adaptor(inputs):\n    # Use one Flatten and One Dense Relu Layer\n    x = tf.keras.layers.Flatten()(inputs)\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    return x\n\ndef build_classifier_head(inputs):\n    # use a Softmax Layer named classifier_head\n    return tf.keras.layers.Dense(CLASSES, activation='softmax', name = 'classifier_head')(inputs)\n\ndef build_regressor_head(inputs):\n    # use a Dense layer with 4 units named regressor_head\n    return tf.keras.layers.Dense(units = 4, name = 'regressor_head')(inputs)\n\ndef build_model(inputs):\n\n    feature_extractor = build_feature_extractor(inputs)\n\n    model_adaptor = build_model_adaptor(feature_extractor)\n\n    classification_head = build_classifier_head(model_adaptor)\n\n    regressor_head = build_regressor_head(model_adaptor)\n\n    model = tf.keras.Model(inputs = inputs, outputs = [classification_head, regressor_head])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.979047Z","iopub.execute_input":"2024-10-19T09:34:24.979525Z","iopub.status.idle":"2024-10-19T09:34:24.989296Z","shell.execute_reply.started":"2024-10-19T09:34:24.979473Z","shell.execute_reply":"2024-10-19T09:34:24.988265Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have everything, Let's start with training!","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"model = build_model(tf.keras.layers.Input(shape=(input_size, input_size, 1,)))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), # Use Adam and set loss and metric for classifier_head and regressor_head as stated earlier\n    loss = {'classifier_head' : 'categorical_crossentropy', 'regressor_head' : 'mse' },\n    metrics = {'classifier_head' : 'accuracy', 'regressor_head' : 'mse' })","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:24.990363Z","iopub.execute_input":"2024-10-19T09:34:24.99064Z","iopub.status.idle":"2024-10-19T09:34:25.057672Z","shell.execute_reply.started":"2024-10-19T09:34:24.990609Z","shell.execute_reply":"2024-10-19T09:34:25.056792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True) # Plot the Model","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:25.058838Z","iopub.execute_input":"2024-10-19T09:34:25.059572Z","iopub.status.idle":"2024-10-19T09:34:25.67907Z","shell.execute_reply.started":"2024-10-19T09:34:25.059529Z","shell.execute_reply":"2024-10-19T09:34:25.677631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 32\n\nhistory = model.fit(train_ds,\n                    steps_per_epoch=(len(training_files) // BATCH_SIZE),\n                    validation_data=validation_ds, validation_steps=1,\n                    epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:34:25.680266Z","iopub.execute_input":"2024-10-19T09:34:25.680555Z","iopub.status.idle":"2024-10-19T09:35:33.712624Z","shell.execute_reply.started":"2024-10-19T09:34:25.680524Z","shell.execute_reply":"2024-10-19T09:35:33.711805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IoU Metric","metadata":{}},{"cell_type":"markdown","source":"IoU scores how well the predicted bound box overlaps the actual bound box. The idea behind IoU is pretty simple: compare the intersection and union areas between the predicted and actual bound boxes by dividing the intersection by the union. IoU provides a higher score always when the predicted bounding box best matches the actual bounding box (also called ground-truth)","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(boxA, boxB):\n\txA = max(boxA[0], boxB[0])\n\tyA = max(boxA[1], boxB[1])\n\txB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n\tyB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1) # Calculate Intersection Area\n\tboxAArea = (boxA[2] + 1) * (boxA[3] + 1) # Calcualate BoxA Area\n\tboxBArea = (boxB[2] + 1) * (boxB[3] + 1) # Calculate BoxB Area\n\tiou = interArea / float(boxAArea + boxBArea - interArea) # Find IOU\n\treturn iou","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:33.713891Z","iopub.execute_input":"2024-10-19T09:35:33.714199Z","iopub.status.idle":"2024-10-19T09:35:33.721536Z","shell.execute_reply.started":"2024-10-19T09:35:33.714166Z","shell.execute_reply":"2024-10-19T09:35:33.720587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 10))\n\ntest_list = list(test_ds.take(20).as_numpy_iterator())\n\nimage, labels = test_list[0]\n\nfor i in range(len(test_list)):\n\n    ax = plt.subplot(4, 5, i + 1)\n    image, labels = test_list[i]\n\n    predictions = model(image)\n\n    predicted_box = predictions[1][0] * input_size\n    predicted_box = tf.cast(predicted_box, tf.int32)\n\n    predicted_label = predictions[0][0]\n\n    image = image[0]\n\n    actual_label = labels[0][0]\n    actual_box = labels[1][0] * input_size\n    actual_box = tf.cast(actual_box, tf.int32)\n\n    image = image.astype(\"float\") * 255.0\n    image = image.astype(np.uint8)\n    image_color = cv.cvtColor(image, cv.COLOR_GRAY2RGB)\n\n    color = (255, 0, 0)\n    # print box red if predicted and actual label do not match\n    if (predicted_label[0] > 0.5 and actual_label[0] > 0) or (predicted_label[0] < 0.5 and actual_label[0] == 0):\n        color = (0, 255, 0)\n\n    img_label = \"unmasked\"\n    if predicted_label[0] > 0.5:\n        img_label = \"masked\"\n\n    predicted_box_n = predicted_box.numpy()\n    cv.rectangle(image_color, predicted_box_n, color, 2)\n    cv.rectangle(image_color, actual_box.numpy(), (0, 0, 255), 2)\n    cv.rectangle(image_color, (predicted_box_n[0], predicted_box_n[1] + predicted_box_n[3] - 20), (predicted_box_n[0] + predicted_box_n[2], predicted_box_n[1] + predicted_box_n[3]), color, -1)\n    cv.putText(image_color, img_label, (predicted_box_n[0] + 5, predicted_box_n[1] + predicted_box_n[3] - 5), cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0))\n\n    IoU = intersection_over_union(predicted_box.numpy(), actual_box.numpy())\n\n    plt.title(\"IoU:\" + format(IoU, '.4f'))\n    plt.imshow(image_color)\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:33.722657Z","iopub.execute_input":"2024-10-19T09:35:33.722954Z","iopub.status.idle":"2024-10-19T09:35:36.50656Z","shell.execute_reply.started":"2024-10-19T09:35:33.722923Z","shell.execute_reply":"2024-10-19T09:35:36.50562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EfficientNetB3","metadata":{}},{"cell_type":"markdown","source":"EfficientNetB3 is a pre-trained convolutional neural network (CNN) architecture designed for efficient image classification. It's known for its high accuracy and low computational cost.","metadata":{}},{"cell_type":"markdown","source":"add this dataset to the inputs\n\nhttps://www.kaggle.com/datasets/hereisburak/pins-face-recognition\n\nload it below","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/pins-face-recognition/105_classes_pins_dataset\"","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:36.507811Z","iopub.execute_input":"2024-10-19T09:35:36.508118Z","iopub.status.idle":"2024-10-19T09:35:36.512507Z","shell.execute_reply.started":"2024-10-19T09:35:36.508085Z","shell.execute_reply":"2024-10-19T09:35:36.511544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## create a train and validation dataset","metadata":{}},{"cell_type":"code","source":"img_height, img_width = 180, 180\nbatch_size = 32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=123,\n    label_mode='categorical',\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:36.513674Z","iopub.execute_input":"2024-10-19T09:35:36.513983Z","iopub.status.idle":"2024-10-19T09:35:40.701873Z","shell.execute_reply.started":"2024-10-19T09:35:36.513951Z","shell.execute_reply":"2024-10-19T09:35:40.701079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_height, img_width = 180,180\nbatch_size = 32\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    label_mode='categorical',\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:40.703182Z","iopub.execute_input":"2024-10-19T09:35:40.703495Z","iopub.status.idle":"2024-10-19T09:35:43.979522Z","shell.execute_reply.started":"2024-10-19T09:35:40.703457Z","shell.execute_reply":"2024-10-19T09:35:43.978789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Model","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.EfficientNetB3(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(180, 180, 3) #image shape here\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:43.980762Z","iopub.execute_input":"2024-10-19T09:35:43.981138Z","iopub.status.idle":"2024-10-19T09:35:48.939205Z","shell.execute_reply.started":"2024-10-19T09:35:43.981093Z","shell.execute_reply":"2024-10-19T09:35:48.938368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Augument the data for better generalisation","metadata":{}},{"cell_type":"code","source":"data_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.2),\n    layers.RandomContrast(0.2),\n    layers.RandomBrightness(0.2),\n    layers.RandomTranslation(0.1, 0.1),\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:48.940797Z","iopub.execute_input":"2024-10-19T09:35:48.941141Z","iopub.status.idle":"2024-10-19T09:35:48.961669Z","shell.execute_reply.started":"2024-10-19T09:35:48.941106Z","shell.execute_reply":"2024-10-19T09:35:48.96068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build the model","metadata":{}},{"cell_type":"code","source":"inputs = layers.Input(shape=(180, 180, 3))\nx = data_augmentation(inputs)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x) ##what is GAP\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\nx = layers.BatchNormalization()(x)\noutputs = layers.Dense(105, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:48.962694Z","iopub.execute_input":"2024-10-19T09:35:48.962994Z","iopub.status.idle":"2024-10-19T09:35:49.03127Z","shell.execute_reply.started":"2024-10-19T09:35:48.962963Z","shell.execute_reply":"2024-10-19T09:35:49.030397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use a custom LR","metadata":{}},{"cell_type":"code","source":"def lr_schedule(epoch):\n    # tweak this around if you want to\n    \n    lr = 1e-3\n    if epoch > 10:\n        lr *= 0.1\n    if epoch > 20:\n        lr *= 0.1\n    return lr","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:49.032372Z","iopub.execute_input":"2024-10-19T09:35:49.03267Z","iopub.status.idle":"2024-10-19T09:35:49.037528Z","shell.execute_reply.started":"2024-10-19T09:35:49.032636Z","shell.execute_reply":"2024-10-19T09:35:49.036582Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compile the model","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:49.038652Z","iopub.execute_input":"2024-10-19T09:35:49.03893Z","iopub.status.idle":"2024-10-19T09:35:49.054558Z","shell.execute_reply.started":"2024-10-19T09:35:49.038901Z","shell.execute_reply":"2024-10-19T09:35:49.053635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using Callbacks\n\nhttps://medium.com/@ompramod9921/callbacks-your-secret-weapon-in-machine-learning-b08ded5678f0","metadata":{}},{"cell_type":"code","source":"initial_epochs = 20\ncallbacks = [\n    keras.callbacks.LearningRateScheduler(lr_schedule),\n    keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy'),\n    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:49.055815Z","iopub.execute_input":"2024-10-19T09:35:49.056437Z","iopub.status.idle":"2024-10-19T09:35:49.062956Z","shell.execute_reply.started":"2024-10-19T09:35:49.056402Z","shell.execute_reply":"2024-10-19T09:35:49.062185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initial Training","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:35:49.063948Z","iopub.execute_input":"2024-10-19T09:35:49.064221Z","iopub.status.idle":"2024-10-19T10:47:55.58065Z","shell.execute_reply.started":"2024-10-19T09:35:49.064191Z","shell.execute_reply":"2024-10-19T10:47:55.579799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tuning\nbase_model.trainable = True\n\n# Freeze batch norm layers\nfor layer in base_model.layers:\n    if isinstance(layer, layers.BatchNormalization):\n        layer.trainable = False\n# Recompile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T10:47:55.582191Z","iopub.execute_input":"2024-10-19T10:47:55.582903Z","iopub.status.idle":"2024-10-19T10:47:55.59746Z","shell.execute_reply.started":"2024-10-19T10:47:55.582855Z","shell.execute_reply":"2024-10-19T10:47:55.596657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the rest and fine tuning","metadata":{}},{"cell_type":"code","source":"fine_tune_epochs = 30\ntotal_epochs = initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=total_epochs,\n    initial_epoch=initial_epochs,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T10:47:55.598695Z","iopub.execute_input":"2024-10-19T10:47:55.599089Z","iopub.status.idle":"2024-10-19T12:03:17.759832Z","shell.execute_reply.started":"2024-10-19T10:47:55.599047Z","shell.execute_reply":"2024-10-19T12:03:17.758812Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport pickle\n\n# Get class names from the dataset\nclass_names = train_ds.class_names\n\n# Create and fit LabelEncoder\nle = LabelEncoder()\nle.fit(class_names)\n\n# Save LabelEncoder\nwith open('label_encoder.pkl', 'wb') as le_file:\n    pickle.dump(le, le_file)\n\nprint(\"LabelEncoder saved successfully.\")\n\nmodel.save('/kaggle/working/efficientnetb3.keras')\nprint(\"Final model saved successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T12:03:17.761269Z","iopub.execute_input":"2024-10-19T12:03:17.761599Z","iopub.status.idle":"2024-10-19T12:03:19.426801Z","shell.execute_reply.started":"2024-10-19T12:03:17.761564Z","shell.execute_reply":"2024-10-19T12:03:19.425633Z"},"trusted":true},"outputs":[],"execution_count":null}]}